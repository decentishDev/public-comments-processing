{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d89bb6b",
   "metadata": {},
   "source": [
    "# Summarizing Key Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b909717",
   "metadata": {},
   "source": [
    "### Getting topics file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64a2f029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WAT': 1001,\n",
       " 'GEN': 846,\n",
       " 'SO': 325,\n",
       " 'FIN': 263,\n",
       " 'PER': 231,\n",
       " 'NEPA': 209,\n",
       " 'ALT': 165,\n",
       " 'WET': 149,\n",
       " 'PD': 148,\n",
       " 'MERC': 132,\n",
       " 'O': 132,\n",
       " 'MEPA': 105,\n",
       " 'HU': 94,\n",
       " 'LAN': 92,\n",
       " 'CUM': 83,\n",
       " 'GT': 78,\n",
       " 'AIR': 72,\n",
       " 'WI': 61,\n",
       " 'CR': 59,\n",
       " 'ROD': 39,\n",
       " 'WILD': 24,\n",
       " 'VEG': 22,\n",
       " 'AQ': 21,\n",
       " 'COE': 18,\n",
       " 'HAZ': 16,\n",
       " 'LU': 15,\n",
       " 'EDIT': 3,\n",
       " 'N': 3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined_responses.csv\")\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "grouped_comments = df.groupby(\"Issue\")[\"Comment\"].apply(lambda comments: list(comments.astype(str)))\n",
    "counts = df[\"Issue\"].value_counts().to_dict()\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3169fe",
   "metadata": {},
   "source": [
    "### Summarizing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f01a8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing Issue ID: AIR\n",
      "Finished in 0.40 seconds\n",
      "\n",
      "Summarizing Issue ID: ALT\n",
      "Finished in 0.96 seconds\n",
      "\n",
      "Summarizing Issue ID: AQ\n",
      "Finished in 0.07 seconds\n",
      "\n",
      "Summarizing Issue ID: COE\n",
      "Finished in 0.07 seconds\n",
      "\n",
      "Summarizing Issue ID: CR\n",
      "Finished in 0.25 seconds\n",
      "\n",
      "Summarizing Issue ID: CUM\n",
      "Finished in 0.34 seconds\n",
      "\n",
      "Summarizing Issue ID: EDIT\n",
      "Finished in 0.01 seconds\n",
      "\n",
      "Summarizing Issue ID: FIN\n",
      "Finished in 0.92 seconds\n",
      "\n",
      "Summarizing Issue ID: GEN\n",
      "Finished in 2.63 seconds\n",
      "\n",
      "Summarizing Issue ID: GT\n",
      "Finished in 0.49 seconds\n",
      "\n",
      "Summarizing Issue ID: HAZ\n",
      "Finished in 0.06 seconds\n",
      "\n",
      "Summarizing Issue ID: HU\n",
      "Finished in 0.26 seconds\n",
      "\n",
      "Summarizing Issue ID: LAN\n",
      "Finished in 0.32 seconds\n",
      "\n",
      "Summarizing Issue ID: LU\n",
      "Finished in 0.05 seconds\n",
      "\n",
      "Summarizing Issue ID: MEPA\n",
      "Finished in 0.26 seconds\n",
      "\n",
      "Summarizing Issue ID: MERC\n",
      "Finished in 0.74 seconds\n",
      "\n",
      "Summarizing Issue ID: N\n",
      "Finished in 0.01 seconds\n",
      "\n",
      "Summarizing Issue ID: NEPA\n",
      "Finished in 1.13 seconds\n",
      "\n",
      "Summarizing Issue ID: O\n",
      "Finished in 0.65 seconds\n",
      "\n",
      "Summarizing Issue ID: PD\n",
      "Finished in 0.46 seconds\n",
      "\n",
      "Summarizing Issue ID: PER\n",
      "Finished in 0.80 seconds\n",
      "\n",
      "Summarizing Issue ID: ROD\n",
      "Finished in 0.10 seconds\n",
      "\n",
      "Summarizing Issue ID: SO\n",
      "Finished in 0.80 seconds\n",
      "\n",
      "Summarizing Issue ID: VEG\n",
      "Finished in 0.06 seconds\n",
      "\n",
      "Summarizing Issue ID: WAT\n",
      "Finished in 4.89 seconds\n",
      "\n",
      "Summarizing Issue ID: WET\n",
      "Finished in 0.78 seconds\n",
      "\n",
      "Summarizing Issue ID: WI\n",
      "Finished in 0.45 seconds\n",
      "\n",
      "Summarizing Issue ID: WILD\n",
      "Finished in 0.05 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_text(text, sentence_count=5):\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LsaSummarizer()\n",
    "    summary = summarizer(parser.document, sentence_count)\n",
    "    return ' '.join(str(sentence) for sentence in summary)\n",
    "\n",
    "def split_comments_into_chunks(comments, max_chars=4096):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for comment in comments:\n",
    "        comment = str(comment).strip()\n",
    "        if not comment:\n",
    "            continue\n",
    "        comment_length = len(comment)\n",
    "        if current_length + comment_length + 1 > max_chars:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [comment]\n",
    "            current_length = comment_length\n",
    "        else:\n",
    "            current_chunk.append(comment)\n",
    "            current_length += comment_length + 1\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    return chunks\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for issue_id, comment_list in grouped_comments.items():\n",
    "    print(f\"Summarizing Issue ID: {issue_id}\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        chunks = split_comments_into_chunks(comment_list, max_chars=4096)\n",
    "        chunk_summaries = []\n",
    "        for chunk in chunks:\n",
    "            result = summarize_text(chunk, sentence_count=5)\n",
    "            chunk_summaries.append(result)\n",
    "        if len(chunk_summaries) == 1:\n",
    "            final_summary = chunk_summaries[0]\n",
    "        else:\n",
    "            joined_summary = \" \".join(chunk_summaries)\n",
    "            final_summary = summarize_text(joined_summary, sentence_count=5)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error summarizing {issue_id}: {e}\")\n",
    "        final_summary = \"[Summary Failed]\"\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished in {end_time - start_time:.2f} seconds\\n\")\n",
    "    summaries.append((issue_id, counts.get(issue_id, 0), final_summary))\n",
    "\n",
    "summary_df = pd.DataFrame(summaries, columns=[\"Issue\", \"Count\", \"Summary\"])\n",
    "summary_df = summary_df.sort_values(by=\"Count\", ascending=False)\n",
    "summary_df.to_csv(\"results/issues_summarized1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
